{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLTK Lab.ipynb","provenance":[],"authorship_tag":"ABX9TyMaN/6J1AEVT0itnIPX1tp8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"_6dWomlv11HQ","executionInfo":{"status":"ok","timestamp":1635256415192,"user_tz":-420,"elapsed":497,"user":{"displayName":"จตุรภัทร จันทร์สีดา","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh8JR5rYFomj1OFkd31vzHVFmEO65yURarvMhL86A=s64","userId":"10274353300587824393"}}},"source":["from nltk.corpus import stopwords \n","from nltk.tokenize import word_tokenize"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":229},"id":"TH51nYN217Q6","executionInfo":{"status":"error","timestamp":1635256417377,"user_tz":-420,"elapsed":5,"user":{"displayName":"จตุรภัทร จันทร์สีดา","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh8JR5rYFomj1OFkd31vzHVFmEO65yURarvMhL86A=s64","userId":"10274353300587824393"}},"outputId":"7bbc5b70-527f-427f-a849-121b37762ddc"},"source":["tweetText = df['Tweets']\n","df[\"Token\"]  = tweetText.apply(word_tokenize)\n","def getstop(Name):\n","  stop_words = set(stopwords.words('english')) \n","  filtered_sentence = [] \n","  for w in Name: \n","    if w not in stop_words: \n","        filtered_sentence.append(w) \n","  return  filtered_sentence"],"execution_count":7,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-f3ba0f0692cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweetText\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Tweets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Token\"\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtweetText\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mfiltered_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"]}]},{"cell_type":"code","metadata":{"id":"BM3a9U7M19V1"},"source":["df[\"stop_words\"]  = df['Token'].apply(getstop)\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C-OmW10s1-ot"},"source":["nltk.download('wordnet')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y1ZNhd8z2DOs"},"source":["from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize \n","from nltk import pos_tag\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5SDbfIDz2GTU"},"source":["def Lemma(comment):\n","  wordnet = WordNetLemmatizer()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KicSXjjV2IW3"},"source":["j = []\n","  for token,tag in pos_tag(comment):\n","    pos=tag[0].lower()\n","    if pos not in ['a','r','n','v']:\n","        pos='n'\n","\n","    j.append(wordnet.lemmatize(token,pos)) \n","  return(j)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5aYrMjd-2Kpk"},"source":["df['Lemmatizetion'] = df['Token'].apply(Lemma)\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kmUxDPBR2PJo"},"source":["from nltk.stem import PorterStemmer \n","def stemming(sent):\n","  ps = PorterStemmer()\n","  ps_sent = [ps.stem(words_sent) for words_sent in sent]\n","  return ps_sent\n","df['Stemming'] = df['Token'].apply(stemming)\n","df"],"execution_count":null,"outputs":[]}]}